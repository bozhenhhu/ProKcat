{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Uni_test/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data from ../data/ .\n",
      "Loading test data from ../data/ .\n",
      "Loading parameters from ../data/hyparams/param_2.pkl .\n",
      "Namespace(batch=1, decay_interval=10, lr=0.001, lr_decay=0.5, num_epoch=5, param_dict_pkl='../data/hyparams/param_2.pkl', seed=42, shuffle_T='False', test_path='../data/', train_path='../data/')\n",
      "{'comp_dim': 80, 'prot_dim': 80, 'gat_dim': 50, 'num_head': 3, 'dropout': 0.1, 'alpha': 0.1, 'window': 5, 'layer_cnn': 4, 'latent_dim': 40, 'layer_out': 4}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from train_functions import *\n",
    "from feature_functions import load_pickle, dump_pickle\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Inputs: --train_path: train data path; --test_path: test data path;\\\n",
    "                        --lr: learning rate;--batch: batch size; --lr_decay:multiplicative factor of learning rate decay. Default: 0.5;\\\n",
    "                        --decay_interval: period of learning rate decay; --num_epoch: the number of epochs;\\\n",
    "                        --param_dict_pkl: the path to parameters; --shuffle_T: shuffle temperature feature if True. Default=False')\n",
    "    \n",
    "parser.add_argument('--train_path', default='../data/')\n",
    "parser.add_argument('--test_path', default='../data/')\n",
    "parser.add_argument('--lr', default = 0.001, type=float )\n",
    "parser.add_argument('--batch', default = 1 , type=int )\n",
    "parser.add_argument('--lr_decay', default = 0.5, type=float )\n",
    "parser.add_argument('--decay_interval', default = 10, type=int )\n",
    "parser.add_argument('--num_epoch', default = 5, type=int )\n",
    "parser.add_argument('--seed', default = 42, type=int )\n",
    "parser.add_argument('--param_dict_pkl', default = '../data/hyparams/param_2.pkl')\n",
    "parser.add_argument('--shuffle_T', default = 'False', choices=['False','True'], type = str )\n",
    "# args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "\n",
    "train_path, test_path, lr, batch_size, lr_decay, decay_interval, param_dict_pkl = \\\n",
    "        str(args.train_path), str(args.test_path), float(args.lr), int(args.batch), \\\n",
    "        float(args.lr_decay), int(args.decay_interval) , str( args.param_dict_pkl )\n",
    "\n",
    "print('Loading train data from %s .' % train_path)\n",
    "if not ( os.path.isdir(train_path) ):\n",
    "    raise SystemExit('Directory %s does not exist!' % train_path )\n",
    "    \n",
    "print('Loading test data from %s .' % test_path)\n",
    "if not ( os.path.isdir(test_path) ):\n",
    "    raise SystemExit('Directory %s does not exist!' % test_path )\n",
    "    \n",
    "print('Loading parameters from %s .' % param_dict_pkl)\n",
    "if not ( os.path.exists(param_dict_pkl) ):\n",
    "    raise SystemExit('File %s does not exist!' % param_dict_pkl )\n",
    "    \n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "print(args)\n",
    "\n",
    "param_dict = load_pickle(param_dict_pkl)\n",
    "comp_dim, prot_dim, gat_dim, num_head, dropout, alpha, window, layer_cnn, latent_dim, layer_out = \\\n",
    "        param_dict['comp_dim'], param_dict['prot_dim'],param_dict['gat_dim'],param_dict['num_head'],\\\n",
    "        param_dict['dropout'], param_dict['alpha'], param_dict['window'], param_dict['layer_cnn'], \\\n",
    "        param_dict['latent_dim'], param_dict['layer_out']\n",
    "\n",
    "print(param_dict)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Setting attributes on ParameterList is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_esm_data( path, has_label, type ):\n",
    "    '''\n",
    "    Load features generated by /code/gen_features.py.\n",
    "    '''\n",
    "    compounds = np.array( load_pickle( os.path.join(path, '{}_features_esm/compounds.pkl'.format(type)) ), dtype=object )\n",
    "    adjacencies = np.array( load_pickle( os.path.join(path, '{}_features_esm/adjacencies.pkl'.format(type)) ), dtype=object )\n",
    "    fps =  np.array( load_pickle( os.path.join(path, '{}_features_esm/fps.pkl'.format(type)) ), dtype=object )\n",
    "    proteins = load_pickle( os.path.join(path, '{}_features_esm/proteins.pkl'.format(type)) )\n",
    "    inv_Temp = np.array( load_pickle( os.path.join(path,  '{}_features_esm/inv_Temp.pkl'.format(type)) ) , dtype=object)\n",
    "    Temp = np.array( load_pickle( os.path.join(path,  '{}_features_esm/Temp.pkl'.format(type)) ) , dtype=object)\n",
    "    if has_label:\n",
    "        targets = np.array( load_pickle( os.path.join(path,  '{}_features_esm/log10_kcat.pkl'.format(type)) ) , dtype=object)\n",
    "        data_pack = [ compounds, adjacencies, fps , proteins, inv_Temp, Temp, targets ]\n",
    "    else:\n",
    "        data_pack = [ compounds, adjacencies, fps , proteins, inv_Temp, Temp ]\n",
    "        \n",
    "    \n",
    "    return data_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data( data, ratio=0.1):\n",
    "    '''\n",
    "    Randomly split data into two datasets.\n",
    "    '''\n",
    "    idx = np.arange(len( data[0]))\n",
    "    np.random.shuffle(idx)\n",
    "    num_split = int(len(data[0]) * ratio)\n",
    "    idx_1, idx_0 = idx[:num_split], idx[num_split:]\n",
    "    data_0 = []\n",
    "    data_1 = []\n",
    "    for di in range(len(data)):\n",
    "        if di != 3:\n",
    "            data_0.append(data[di][idx_0])\n",
    "            data_1.append(data[di][idx_1])\n",
    "        if di == 3:\n",
    "            tmp1 = []\n",
    "            tmp2 = []\n",
    "            idx_0 = list(idx_0)\n",
    "            idx_1 = list(idx_1)\n",
    "            for idx in idx_0:\n",
    "                tmp1.append(data[di][idx])\n",
    "            for idx in idx_1:\n",
    "                tmp2.append(data[di][idx])\n",
    "            data_0.append(tmp1)\n",
    "            data_1.append(tmp2)\n",
    "    return data_0, data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_dict = load_pickle(  '../data/dict/fingerprint_dict.pkl' )\n",
    "word_dict = load_pickle(   '../data/dict/word_dict.pkl' )\n",
    "\n",
    "datapack = load_esm_data(train_path, True, 'train')\n",
    "test_data = load_esm_data(test_path, True, 'test')\n",
    "\n",
    "train_data, dev_data = split_data( datapack, 0.1 )\n",
    "\n",
    "num_epochs = int( args.num_epoch )#fixed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout=0.5, alpha=0.2, concat=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.matmul(h, self.W)\n",
    "        a_input = self._prepare_attentional_mechanism_input(Wh)\n",
    "        e = F.leaky_relu(torch.matmul(a_input, self.a).squeeze(3), self.alpha)\n",
    "\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=2)\n",
    "        # attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.bmm(attention, Wh)\n",
    "\n",
    "        return F.elu(h_prime) if self.concat else h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        b = Wh.size()[0]\n",
    "        N = Wh.size()[1]\n",
    "\n",
    "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=1)\n",
    "        Wh_repeated_alternating = Wh.repeat_interleave(N, dim=0).view(b, N*N, self.out_features)\n",
    "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=2)\n",
    "\n",
    "        return all_combinations_matrix.view(b, N, N, 2 * self.out_features)\n",
    "\n",
    "\n",
    "class KcatModel(nn.Module):\n",
    "    def __init__(self, n_atom, n_amino, comp_dim, prot_dim, gat_dim, num_head, dropout, alpha, window, layer_cnn, latent_dim, layer_out ):\n",
    "        super(KcatModel, self).__init__()\n",
    "        '''\n",
    "        n_atom here stands for number of atom_features\n",
    "        '''\n",
    "\n",
    "\n",
    "        self.embedding_layer_atom = nn.Embedding(n_atom+1, comp_dim)\n",
    "        self.embedding_layer_amino = nn.Embedding(n_amino+1, prot_dim)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.layer_cnn = layer_cnn\n",
    "        self.latent_dim = latent_dim\n",
    "        self.layer_out = layer_out\n",
    "\n",
    "        self.gat_layers = [GATLayer(comp_dim, gat_dim, dropout=dropout, alpha=alpha, concat=True)\n",
    "                           for _ in range(num_head)]\n",
    "        for i, layer in enumerate(self.gat_layers):\n",
    "            self.add_module('gat_layer_{}'.format(i), layer)\n",
    "        self.gat_out = GATLayer(gat_dim * num_head, comp_dim, dropout=dropout, alpha=alpha, concat=False)\n",
    "        self.W_comp = nn.Linear(comp_dim, latent_dim)\n",
    "\n",
    "        self.conv_layers = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2*window+1,\n",
    "                                                    stride=1, padding=window) for _ in range(layer_cnn)])\n",
    "        self.W_prot = nn.Linear(prot_dim, latent_dim)\n",
    "\n",
    "        self.fp0 = nn.Parameter(torch.empty(size=(1024, latent_dim)))\n",
    "        nn.init.xavier_uniform_(self.fp0, gain=1.414)\n",
    "        self.fp1 = nn.Parameter(torch.empty(size=(latent_dim, latent_dim)))\n",
    "        nn.init.xavier_uniform_(self.fp1, gain=1.414)\n",
    "\n",
    "        self.bidat_num = 4\n",
    "\n",
    "        self.U = nn.ParameterList([nn.Parameter(torch.empty(size=(latent_dim, latent_dim))) for _ in range(self.bidat_num)])\n",
    "        for i in range(self.bidat_num):\n",
    "            nn.init.xavier_uniform_(self.U[i], gain=1.414)\n",
    "\n",
    "        self.transform_c2p = nn.ModuleList([nn.Linear(latent_dim, latent_dim) for _ in range(self.bidat_num)])\n",
    "        self.transform_p2c = nn.ModuleList([nn.Linear(latent_dim, latent_dim) for _ in range(self.bidat_num)])\n",
    "\n",
    "        self.bihidden_c = nn.ModuleList([nn.Linear(latent_dim, latent_dim) for _ in range(self.bidat_num)])\n",
    "        self.bihidden_p = nn.ModuleList([nn.Linear(latent_dim, latent_dim) for _ in range(self.bidat_num)])\n",
    "        self.biatt_c = nn.ModuleList([nn.Linear(latent_dim * 2, 1) for _ in range(self.bidat_num)])\n",
    "        self.biatt_p = nn.ModuleList([nn.Linear(latent_dim * 2, 1) for _ in range(self.bidat_num)])\n",
    "\n",
    "        self.comb_c = nn.Linear(latent_dim * self.bidat_num, latent_dim)\n",
    "        self.comb_p = nn.Linear(latent_dim * self.bidat_num, latent_dim)\n",
    "   \n",
    "        self.W_out = nn.ModuleList([nn.Linear(latent_dim * 3 + 2, latent_dim * 3 + 2)\n",
    "                                    for _ in range(self.layer_out)])\n",
    "    \n",
    "        self.output = nn.Linear(latent_dim * 3 + 2, 1)\n",
    "        \n",
    "\n",
    "    def comp_gat(self, atoms, adj):\n",
    "        atoms_vector = self.embedding_layer_atom(atoms)\n",
    "        atoms_multi_head = torch.cat([gat(atoms_vector, adj) for gat in self.gat_layers], dim=2)\n",
    "        atoms_vector = F.elu(self.gat_out(atoms_multi_head, adj))\n",
    "        atoms_vector = F.leaky_relu(self.W_comp(atoms_vector), self.alpha)\n",
    "        return atoms_vector #(B,L, 40)\n",
    "\n",
    "    def prot_cnn(self, amino ):\n",
    "        amino_vector = self.embedding_layer_amino(amino)\n",
    "        amino_vector = torch.unsqueeze(amino_vector, 1)\n",
    "        for i in range(self.layer_cnn):\n",
    "            amino_vector = F.leaky_relu(self.conv_layers[i](amino_vector), self.alpha)\n",
    "        amino_vector = torch.squeeze(amino_vector, 1)\n",
    "        amino_vector = F.leaky_relu(self.W_prot(amino_vector), self.alpha)\n",
    "        return amino_vector\n",
    "\n",
    "    def mask_softmax(self, a, mask, dim=-1):\n",
    "        a_max = torch.max(a, dim, keepdim=True)[0]\n",
    "        a_exp = torch.exp(a - a_max)\n",
    "        a_exp = a_exp * mask\n",
    "        a_softmax = a_exp / (torch.sum(a_exp, dim, keepdim=True) + 1e-6)\n",
    "        return a_softmax\n",
    "\n",
    "    def bidirectional_attention_prediction(self,atoms_vector, atoms_mask, fps, amino_vector, amino_mask, inv_Temp, Temp):\n",
    "        b = atoms_vector.shape[0]\n",
    "        for i in range(self.bidat_num):\n",
    "            A = torch.tanh(torch.matmul(torch.matmul(atoms_vector, self.U[i]), amino_vector.transpose(1, 2)))\n",
    "            A = A * torch.matmul(atoms_mask.view(b, -1, 1), amino_mask.view(b, 1, -1))\n",
    "\n",
    "            atoms_trans = torch.matmul(A, torch.tanh(self.transform_p2c[i](amino_vector)))\n",
    "            amino_trans = torch.matmul(A.transpose(1, 2), torch.tanh(self.transform_c2p[i](atoms_vector)))\n",
    "\n",
    "            atoms_tmp = torch.cat([torch.tanh(self.bihidden_c[i](atoms_vector)), atoms_trans], dim=2)\n",
    "            amino_tmp = torch.cat([torch.tanh(self.bihidden_p[i](amino_vector)), amino_trans], dim=2)\n",
    "\n",
    "            atoms_att = self.mask_softmax(self.biatt_c[i](atoms_tmp).view(b, -1), atoms_mask.view(b, -1))\n",
    "            amino_att = self.mask_softmax(self.biatt_p[i](amino_tmp).view(b, -1), amino_mask.view(b, -1))\n",
    "\n",
    "            cf = torch.sum(atoms_vector * atoms_att.view(b, -1, 1), dim=1)\n",
    "            pf = torch.sum(amino_vector * amino_att.view(b, -1, 1), dim=1)\n",
    "\n",
    "            if i == 0:\n",
    "                cat_cf = cf\n",
    "                cat_pf = pf\n",
    "            else:\n",
    "                cat_cf = torch.cat([cat_cf.view(b, -1), cf.view(b, -1)], dim=1)\n",
    "                cat_pf = torch.cat([cat_pf.view(b, -1), pf.view(b, -1)], dim=1)\n",
    "\n",
    "        inverse_Temp = inv_Temp.view(inv_Temp.shape[0],-1)\n",
    "        Temperature = Temp.view(Temp.shape[0],-1)\n",
    "        cf_final = torch.cat([self.comb_c(cat_cf).view(b, -1), fps.view(b, -1)], dim=1)#length = 2*d\n",
    "        pf_final = self.comb_p(cat_pf)#length = d = 40\n",
    "        cat_vector = torch.cat((cf_final, pf_final, inverse_Temp, Temperature), dim=1)#length=3*d+2\n",
    "        \n",
    "        for j in range(self.layer_out):\n",
    "            cat_vector = F.leaky_relu(self.W_out[j](cat_vector), self.alpha )\n",
    "            \n",
    "        return self.output(cat_vector)\n",
    "\n",
    "    def forward(self, atoms, atoms_mask, adjacency, amino_vector, amino_mask, fps, inv_Temp, Temp ):\n",
    "        atoms_vector = self.comp_gat(atoms, adjacency)\n",
    "        # amino_vector = self.prot_cnn( amino )\n",
    "\n",
    "        super_feature = F.leaky_relu(torch.matmul(fps, self.fp0), 0.1)\n",
    "        super_feature = F.leaky_relu(torch.matmul(super_feature, self.fp1), 0.1)\n",
    "\n",
    "        prediction = self.bidirectional_attention_prediction( atoms_vector, atoms_mask, super_feature,\\\n",
    "                                                             amino_vector, amino_mask, inv_Temp, Temp )\n",
    "        \n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_pad(arr):\n",
    "    '''\n",
    "    Pad feature vectors all into the same length.\n",
    "    '''\n",
    "    N = max([a.shape[0] for a in arr])\n",
    "    if arr[0].ndim == 1:\n",
    "        new_arr = np.zeros((len(arr), N))\n",
    "        new_arr_mask = np.zeros((len(arr), N))\n",
    "        for i, a in enumerate(arr):\n",
    "            n = a.shape[0]\n",
    "            new_arr[i, :n] = a + 1\n",
    "            new_arr_mask[i, :n] = 1\n",
    "        return new_arr, new_arr_mask\n",
    "\n",
    "    elif arr[0].ndim == 2:\n",
    "        new_arr = np.zeros((len(arr), N, N))\n",
    "        new_arr_mask = np.zeros((len(arr), N, N))\n",
    "        for i, a in enumerate(arr):\n",
    "            n = a.shape[0]\n",
    "            new_arr[i, :n, :n] = a\n",
    "            new_arr_mask[i, :n, :n] = 1\n",
    "        return new_arr, new_arr_mask\n",
    "\n",
    "def batch2tensor(batch_data, has_label, device):\n",
    "    '''\n",
    "    Convert loaded data into torch tensors.\n",
    "    '''\n",
    "    atoms_pad, atoms_mask = batch_pad(batch_data[0])\n",
    "    adjacencies_pad, _ = batch_pad(batch_data[1])\n",
    "    \n",
    "    fps = batch_data[2]\n",
    "    temp_arr = np.zeros((len(fps), 1024))\n",
    "    for i,a in enumerate(fps):\n",
    "        temp_arr[i, :] = np.array(list(a), dtype=int)\n",
    "    fps = temp_arr\n",
    "    \n",
    "    amino_pad = batch_data[3][0]\n",
    "    amino_mask = np.ones((1, len(amino_pad[0])))\n",
    "    \n",
    "    atoms_pad = Variable(torch.LongTensor(atoms_pad)).to(device)\n",
    "    atoms_mask = Variable(torch.FloatTensor(atoms_mask)).to(device)\n",
    "    adjacencies_pad = Variable(torch.LongTensor(adjacencies_pad)).to(device)\n",
    "    fps = Variable(torch.FloatTensor(fps)).to(device)\n",
    "    amino_pad = Variable(torch.Tensor(amino_pad)).to(device)\n",
    "    amino_mask = Variable(torch.FloatTensor(amino_mask)).to(device)\n",
    "\n",
    "    inv_Temp = batch_data[4]\n",
    "    temp_arr = np.zeros((len(inv_Temp), 1))\n",
    "    for i,a in enumerate( inv_Temp ):\n",
    "        temp_arr[i, :] = a\n",
    "    inv_Temp = torch.FloatTensor(temp_arr).to(device)\n",
    "    \n",
    "    Temp = batch_data[5]\n",
    "    temp_arr = np.zeros((len(Temp), 1))\n",
    "    for i,a in enumerate( Temp ):\n",
    "        temp_arr[i, :] = a\n",
    "    Temp = torch.FloatTensor(temp_arr).to(device)\n",
    "    \n",
    "    if has_label == False:\n",
    "        return atoms_pad, atoms_mask, adjacencies_pad, fps, amino_pad, amino_mask, inv_Temp, Temp\n",
    "    else:\n",
    "        label = batch_data[6]\n",
    "        temp_arr = np.zeros((len(label), 1))\n",
    "        for i,a in enumerate( label ):\n",
    "            temp_arr[i, :] = a\n",
    "        label = torch.FloatTensor(temp_arr).to(device)\n",
    "\n",
    "    return atoms_pad, atoms_mask, adjacencies_pad, fps, amino_pad, amino_mask, inv_Temp, Temp, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alignment(nn.Module):\n",
    "    def __init__(self, n_atom, n_amino, comp_dim, prot_dim, gat_dim, num_head, dropout, alpha, window, layer_cnn, latent_dim, layer_out ):\n",
    "        super(Alignment, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        latent_dim = 40\n",
    "        self.W_a_1 = nn.Linear(1280, latent_dim) \n",
    "        self.W_a_2 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.W_a_3 = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "        self.output = nn.Linear(latent_dim, 1)\n",
    "\n",
    "    def forward(self, amino_vector):\n",
    "        #plan_3: different MLPs for different tensors\n",
    "        # amino_vector = amino.unsqueeze(0)\n",
    "\n",
    "        amino_vector = F.leaky_relu(self.W_a_1(amino_vector), self.alpha ) \n",
    "        amino_vector = F.leaky_relu(self.W_a_2(amino_vector), self.alpha )\n",
    "        amino_vector = F.leaky_relu(self.W_a_3(amino_vector), self.alpha ) \n",
    "\n",
    "        size = amino_vector.size(1)\n",
    "        return self.output(torch.mean(amino_vector[:, 1:size-1, :], dim=1)), amino_vector\n",
    "\n",
    "def test(model, model_mlp, data_test, batch_size, device):\n",
    "    model_mlp.eval()\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    idx = [i for i in range(len(data_test[0]))]\n",
    "    min_size = 1   #4\n",
    "    \n",
    "    for i in range(math.ceil(len(data_test[0]) / min_size)):\n",
    "        batch_data = []\n",
    "        for di in range(len(data_test)):\n",
    "            if di != 3:\n",
    "                batch_data.append(data_test[di][idx[ i* min_size: (i + 1) * min_size]])\n",
    "            else:\n",
    "                tmp = []\n",
    "                for id in idx[ i* min_size: (i + 1) * min_size]:\n",
    "                    tmp.append(data_test[di][id])\n",
    "                batch_data.append(tmp)\n",
    "                    \n",
    "        # batch_data = [data_test[di][i * batch_size: (i + 1) * batch_size] for di in range(len(data_test))]\n",
    "        atoms_pad, atoms_mask, adjacencies_pad, batch_fps, amino_pad,\\\n",
    "                            amino_mask, inv_Temp, Temp, label = batch2tensor(batch_data, True, device)\n",
    "        with torch.no_grad():\n",
    "            # pred, _ = model_mlp(amino_pad)\n",
    "            pred, amino_pad = model_mlp(amino_pad)\n",
    "            pred = model( atoms_pad, atoms_mask, adjacencies_pad, amino_pad, amino_mask, batch_fps, inv_Temp, Temp )\n",
    "            \n",
    "        predictions += pred.cpu().detach().numpy().reshape(-1).tolist()\n",
    "        labels += label.cpu().numpy().reshape(-1).tolist()\n",
    "        \n",
    "    predictions = np.array(predictions)\n",
    "    labels = np.array(labels)\n",
    "    rmse, r2 = scores(labels, predictions)\n",
    "    \n",
    "    return rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(model, model_mlp, data_train, data_test, data_dev, device, lr, batch_size, lr_decay, decay_interval, num_epochs ):\n",
    "    criterion = F.mse_loss\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0, amsgrad=True)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size= decay_interval, gamma=lr_decay)\n",
    "    idx = np.arange(len(data_train[0]))\n",
    "    \n",
    "    min_size = 1   #4\n",
    "    if batch_size > min_size:\n",
    "        div_min = int(batch_size / min_size)\n",
    "    else:\n",
    "        div_min = 1\n",
    "        \n",
    "    rmse_train_scores, r2_train_scores, rmse_test_scores, r2_test_scores, rmse_dev_scores, r2_dev_scores = [],[],[],[],[],[]\n",
    "    for epoch in range(num_epochs):\n",
    "             \n",
    "        np.random.shuffle(idx)\n",
    "        model.train()\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        for i in range(math.ceil( len(data_train[0]) / min_size )):\n",
    "            batch_data = []\n",
    "            for di in range(len(data_train)):\n",
    "                if di != 3:\n",
    "                    batch_data.append(data_train[di][idx[ i* min_size: (i + 1) * min_size]])\n",
    "                else:\n",
    "                    tmp = []\n",
    "                    for id in idx[ i* min_size: (i + 1) * min_size]:\n",
    "                        tmp.append(data_train[di][id])\n",
    "                    batch_data.append(tmp)\n",
    "            # batch_data = [data_train[di][idx[i]] \\\n",
    "            #               for di in range(len(data_train))]\n",
    "            atoms_pad, atoms_mask, adjacencies_pad, batch_fps, amino_pad,\\\n",
    "                            amino_mask, inv_Temp, Temp, label = batch2tensor(batch_data, True, device)\n",
    "            \n",
    "            # pred, _ = model_mlp(amino_pad)\n",
    "            pred, amino_pad = model_mlp(amino_pad)\n",
    "            pred = model( atoms_pad, atoms_mask, adjacencies_pad, amino_pad, amino_mask, batch_fps, inv_Temp, Temp )\n",
    "            loss = criterion(pred.float(), label.float())\n",
    "            predictions += pred.cpu().detach().numpy().reshape(-1).tolist()\n",
    "            labels += label.cpu().numpy().reshape(-1).tolist()\n",
    "            loss.backward()\n",
    "            if i % div_min == 0 and i != 0:    \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        labels = np.array(labels)\n",
    "        rmse_train, r2_train = scores( labels, predictions )\n",
    "        rmse_dev, r2_dev = test( model, model_mlp, data_dev, batch_size, device ) #dev dataset\n",
    "        rmse_test, r2_test = test(model, model_mlp, data_test, batch_size, device) # test dataset\n",
    "        \n",
    "        if rmse_test < 0.91:\n",
    "            print('Best model found at epoch=' + str(epoch) + '!')\n",
    "            best_model_pth = '../data/my_performance/model_latentdim=' + str(model.latent_dim) + '_outlayer=' + str(model.layer_out)\n",
    "            best_model_pth = best_model_pth + '_rmsetest='+str( round(rmse_test,4) )+'_rmsedev='+str( round(rmse_dev,4) ) +'.pth'\n",
    "            torch.save( model.state_dict(), best_model_pth)\n",
    "            best_model_pth = '../data/my_performance/model_mlp_latentdim=' + str(model.latent_dim) + '_outlayer=' + str(model.layer_out)\n",
    "            best_model_pth = best_model_pth + '_rmsetest='+str( round(rmse_test,4) )+'_rmsedev='+str( round(rmse_dev,4) ) +'.pth'\n",
    "            torch.save( model_mlp.state_dict(), best_model_pth)\n",
    "\n",
    "\n",
    "        rmse_train_scores.append( rmse_train )\n",
    "        r2_train_scores.append( r2_train )\n",
    "        rmse_dev_scores.append( rmse_dev )\n",
    "        r2_dev_scores.append( r2_dev )\n",
    "        rmse_test_scores.append( rmse_test )\n",
    "        r2_test_scores.append( r2_test )\n",
    "\n",
    "        \n",
    "        print('epoch: '+str(epoch)+'/'+ str(num_epochs) +';  rmse test: ' + str(rmse_test) + '; r2 test: ' + str(r2_test) )\n",
    "        \n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "    return rmse_train_scores, r2_train_scores, rmse_test_scores, r2_test_scores, rmse_dev_scores, r2_dev_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/1;  rmse test: 1.327613; r2 test: 0.247503\n"
     ]
    }
   ],
   "source": [
    "model = KcatModel( len(atom_dict), len(word_dict), comp_dim, prot_dim, gat_dim, num_head, \\\n",
    "                          dropout, alpha, window, layer_cnn, latent_dim, layer_out )\n",
    "model.to(device)\n",
    "# model.load_state_dict(torch.load('../data/performances/model_latentdim=40_outlayer=4_rmsetest=0.8854_rmsedev=0.908.pth', map_location=device))\n",
    "\n",
    "\n",
    "model_mlp = Alignment( len(atom_dict), len(word_dict), comp_dim, prot_dim, gat_dim, num_head, \\\n",
    "                          dropout, alpha, window, layer_cnn, latent_dim, layer_out )\n",
    "model_mlp.to(device)\n",
    "rmse_train_scores, r2_train_scores, rmse_test_scores, r2_test_scores ,rmse_dev_scores, r2_dev_scores = \\\n",
    "        train_eval(model, model_mlp , train_data, test_data, dev_data, device, lr, batch_size, lr_decay,\\\n",
    "                   decay_interval, num_epochs=1) #num_epochs=40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_inds = list(np.arange(1,num_epochs+1))\n",
    "\n",
    "result = pd.DataFrame(zip( epoch_inds,rmse_train_scores, r2_train_scores, rmse_test_scores, r2_test_scores, \\\n",
    "                                rmse_dev_scores, r2_dev_scores ),\\\n",
    "                        columns = ['epoch','RMSE_train','R2_train','RMSE_test','R2_test','RMSE_dev','R2_dev'])\n",
    "\n",
    "output_path = os.path.join(  '../data/my_performance/',os.path.basename(param_dict_pkl).split('.')[0] + \\\n",
    "                        '_lr=' + str(lr) + '_batch='+str(batch_size)+ \\\n",
    "                        '_lr_decay=' + str(lr_decay) + '_decay_interval=' + str(decay_interval) + \\\n",
    "                        '_shuffle_T=False'+'.csv' )\n",
    "\n",
    "result.to_csv(output_path,index=None)\n",
    "print('Done for ' + param_dict_pkl +'.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Uni_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
