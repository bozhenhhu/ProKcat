{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Uni_test/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data from /usr/data/DLTKcat/data/ .\n",
      "Loading test data from /usr/data/DLTKcat/data/ .\n",
      "Loading parameters from /usr/data/DLTKcat/data/hyparams/param_2.pkl .\n",
      "Namespace(batch=8, decay_interval=10, lr=0.001, lr_decay=0.5, num_epoch=1, param_dict_pkl='/usr/data/DLTKcat/data/hyparams/param_2.pkl', seed=42, shuffle_T='False', test_path='/usr/data/DLTKcat/data/', train_path='/usr/data/DLTKcat/data/')\n",
      "{'comp_dim': 80, 'prot_dim': 80, 'gat_dim': 50, 'num_head': 3, 'dropout': 0.1, 'alpha': 0.1, 'window': 5, 'layer_cnn': 4, 'latent_dim': 40, 'layer_out': 4}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from train_functions import *\n",
    "from feature_functions import load_pickle, dump_pickle\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Inputs: --train_path: train data path; --test_path: test data path;\\\n",
    "                        --lr: learning rate;--batch: batch size; --lr_decay:multiplicative factor of learning rate decay. Default: 0.5;\\\n",
    "                        --decay_interval: period of learning rate decay; --num_epoch: the number of epochs;\\\n",
    "                        --param_dict_pkl: the path to parameters; --shuffle_T: shuffle temperature feature if True. Default=False')\n",
    "    \n",
    "parser.add_argument('--train_path', default='/usr/data/DLTKcat/data/')\n",
    "parser.add_argument('--test_path', default='/usr/data/DLTKcat/data/')\n",
    "parser.add_argument('--lr', default = 0.001, type=float )\n",
    "parser.add_argument('--batch', default = 8 , type=int )\n",
    "parser.add_argument('--lr_decay', default = 0.5, type=float )\n",
    "parser.add_argument('--decay_interval', default = 10, type=int )\n",
    "parser.add_argument('--num_epoch', default = 1, type=int )\n",
    "parser.add_argument('--seed', default = 42, type=int )\n",
    "parser.add_argument('--param_dict_pkl', default = '/usr/data/DLTKcat/data/hyparams/param_2.pkl')\n",
    "parser.add_argument('--shuffle_T', default = 'False', choices=['False','True'], type = str )\n",
    "# args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "\n",
    "train_path, test_path, lr, batch_size, lr_decay, decay_interval, param_dict_pkl = \\\n",
    "        str(args.train_path), str(args.test_path), float(args.lr), int(args.batch), \\\n",
    "        float(args.lr_decay), int(args.decay_interval) , str( args.param_dict_pkl )\n",
    "\n",
    "print('Loading train data from %s .' % train_path)\n",
    "if not ( os.path.isdir(train_path) ):\n",
    "    raise SystemExit('Directory %s does not exist!' % train_path )\n",
    "    \n",
    "print('Loading test data from %s .' % test_path)\n",
    "if not ( os.path.isdir(test_path) ):\n",
    "    raise SystemExit('Directory %s does not exist!' % test_path )\n",
    "    \n",
    "print('Loading parameters from %s .' % param_dict_pkl)\n",
    "if not ( os.path.exists(param_dict_pkl) ):\n",
    "    raise SystemExit('File %s does not exist!' % param_dict_pkl )\n",
    "    \n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "print(args)\n",
    "\n",
    "param_dict = load_pickle(param_dict_pkl)\n",
    "comp_dim, prot_dim, gat_dim, num_head, dropout, alpha, window, layer_cnn, latent_dim, layer_out = \\\n",
    "        param_dict['comp_dim'], param_dict['prot_dim'],param_dict['gat_dim'],param_dict['num_head'],\\\n",
    "        param_dict['dropout'], param_dict['alpha'], param_dict['window'], param_dict['layer_cnn'], \\\n",
    "        param_dict['latent_dim'], param_dict['layer_out']\n",
    "\n",
    "print(param_dict)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Setting attributes on ParameterList is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_dict = load_pickle(  '/usr/data/DLTKcat/data/dict/fingerprint_dict.pkl' )\n",
    "word_dict = load_pickle(   '/usr/data/DLTKcat/data/dict/word_dict.pkl' )\n",
    "\n",
    "datapack = load_data(train_path, True, 'train')\n",
    "test_data = load_data(test_path, True, 'test')\n",
    "\n",
    "train_data, dev_data = split_data( datapack, 0.1 )\n",
    "\n",
    "num_epochs = int( args.num_epoch )#fixed value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout=0.5, alpha=0.2, concat=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.matmul(h, self.W)\n",
    "        a_input = self._prepare_attentional_mechanism_input(Wh)\n",
    "        e = F.leaky_relu(torch.matmul(a_input, self.a).squeeze(3), self.alpha)\n",
    "\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=2)\n",
    "        # attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.bmm(attention, Wh)\n",
    "\n",
    "        return F.elu(h_prime) if self.concat else h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        b = Wh.size()[0]\n",
    "        N = Wh.size()[1]\n",
    "\n",
    "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=1)\n",
    "        Wh_repeated_alternating = Wh.repeat_interleave(N, dim=0).view(b, N*N, self.out_features)\n",
    "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=2)\n",
    "\n",
    "        return all_combinations_matrix.view(b, N, N, 2 * self.out_features)\n",
    "\n",
    "\n",
    "class DLTKcat(nn.Module):\n",
    "    def __init__(self, n_atom, n_amino, comp_dim, prot_dim, gat_dim, num_head, dropout, alpha, window, layer_cnn, latent_dim, layer_out ):\n",
    "        super(DLTKcat, self).__init__()\n",
    "        '''\n",
    "        n_atom here stands for number of atom_features\n",
    "        '''\n",
    "\n",
    "\n",
    "        self.embedding_layer_atom = nn.Embedding(n_atom+1, comp_dim)\n",
    "        self.embedding_layer_amino = nn.Embedding(n_amino+1, prot_dim)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.layer_cnn = layer_cnn\n",
    "        self.latent_dim = latent_dim\n",
    "        self.layer_out = layer_out\n",
    "\n",
    "        self.gat_layers = [GATLayer(comp_dim, gat_dim, dropout=dropout, alpha=alpha, concat=True)\n",
    "                           for _ in range(num_head)]\n",
    "        for i, layer in enumerate(self.gat_layers):\n",
    "            self.add_module('gat_layer_{}'.format(i), layer)\n",
    "        self.gat_out = GATLayer(gat_dim * num_head, comp_dim, dropout=dropout, alpha=alpha, concat=False)\n",
    "        self.W_comp = nn.Linear(comp_dim, latent_dim)\n",
    "\n",
    "        self.conv_layers = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2*window+1,\n",
    "                                                    stride=1, padding=window) for _ in range(layer_cnn)])\n",
    "        self.W_prot = nn.Linear(prot_dim, latent_dim)\n",
    "\n",
    "        self.fp0 = nn.Parameter(torch.empty(size=(1024, latent_dim)))\n",
    "        nn.init.xavier_uniform_(self.fp0, gain=1.414)\n",
    "        self.fp1 = nn.Parameter(torch.empty(size=(latent_dim, latent_dim)))\n",
    "        nn.init.xavier_uniform_(self.fp1, gain=1.414)\n",
    "\n",
    "        self.bidat_num = 4\n",
    "\n",
    "        self.U = nn.ParameterList([nn.Parameter(torch.empty(size=(latent_dim, latent_dim))) for _ in range(self.bidat_num)])\n",
    "        for i in range(self.bidat_num):\n",
    "            nn.init.xavier_uniform_(self.U[i], gain=1.414)\n",
    "\n",
    "        self.transform_c2p = nn.ModuleList([nn.Linear(latent_dim, latent_dim) for _ in range(self.bidat_num)])\n",
    "        self.transform_p2c = nn.ModuleList([nn.Linear(latent_dim, latent_dim) for _ in range(self.bidat_num)])\n",
    "\n",
    "        self.bihidden_c = nn.ModuleList([nn.Linear(latent_dim, latent_dim) for _ in range(self.bidat_num)])\n",
    "        self.bihidden_p = nn.ModuleList([nn.Linear(latent_dim, latent_dim) for _ in range(self.bidat_num)])\n",
    "        self.biatt_c = nn.ModuleList([nn.Linear(latent_dim * 2, 1) for _ in range(self.bidat_num)])\n",
    "        self.biatt_p = nn.ModuleList([nn.Linear(latent_dim * 2, 1) for _ in range(self.bidat_num)])\n",
    "\n",
    "        self.comb_c = nn.Linear(latent_dim * self.bidat_num, latent_dim)\n",
    "        self.comb_p = nn.Linear(latent_dim * self.bidat_num, latent_dim)\n",
    "   \n",
    "        self.W_out = nn.ModuleList([nn.Linear(latent_dim * 3 + 2, latent_dim * 3 + 2)\n",
    "                                    for _ in range(self.layer_out)])\n",
    "    \n",
    "        self.output = nn.Linear(latent_dim * 3 + 2, 1)\n",
    "\n",
    "\n",
    "    def comp_gat(self, atoms, adj):\n",
    "        atoms_vector = self.embedding_layer_atom(atoms)\n",
    "        atoms_multi_head = torch.cat([gat(atoms_vector, adj) for gat in self.gat_layers], dim=2)\n",
    "        atoms_vector = F.elu(self.gat_out(atoms_multi_head, adj))\n",
    "        atoms_vector = F.leaky_relu(self.W_comp(atoms_vector), self.alpha)\n",
    "        return atoms_vector #(B,L, 40)\n",
    "\n",
    "    def prot_cnn(self, amino ):\n",
    "        amino_vector = self.embedding_layer_amino(amino)\n",
    "        amino_vector = torch.unsqueeze(amino_vector, 1)\n",
    "        for i in range(self.layer_cnn):\n",
    "            amino_vector = F.leaky_relu(self.conv_layers[i](amino_vector), self.alpha)\n",
    "        amino_vector = torch.squeeze(amino_vector, 1)\n",
    "        amino_vector = F.leaky_relu(self.W_prot(amino_vector), self.alpha)\n",
    "        return amino_vector\n",
    "\n",
    "    def mask_softmax(self, a, mask, dim=-1):\n",
    "        a_max = torch.max(a, dim, keepdim=True)[0]\n",
    "        a_exp = torch.exp(a - a_max)\n",
    "        a_exp = a_exp * mask\n",
    "        a_softmax = a_exp / (torch.sum(a_exp, dim, keepdim=True) + 1e-6)\n",
    "        return a_softmax\n",
    "\n",
    "    def bidirectional_attention_prediction(self,atoms_vector, atoms_mask, fps, amino_vector, amino_mask, inv_Temp, Temp):\n",
    "        b = atoms_vector.shape[0]\n",
    "        for i in range(self.bidat_num):\n",
    "            A = torch.tanh(torch.matmul(torch.matmul(atoms_vector, self.U[i]), amino_vector.transpose(1, 2)))\n",
    "            A = A * torch.matmul(atoms_mask.view(b, -1, 1), amino_mask.view(b, 1, -1))\n",
    "\n",
    "            atoms_trans = torch.matmul(A, torch.tanh(self.transform_p2c[i](amino_vector)))\n",
    "            amino_trans = torch.matmul(A.transpose(1, 2), torch.tanh(self.transform_c2p[i](atoms_vector)))\n",
    "\n",
    "            atoms_tmp = torch.cat([torch.tanh(self.bihidden_c[i](atoms_vector)), atoms_trans], dim=2)\n",
    "            amino_tmp = torch.cat([torch.tanh(self.bihidden_p[i](amino_vector)), amino_trans], dim=2)\n",
    "\n",
    "            atoms_att = self.mask_softmax(self.biatt_c[i](atoms_tmp).view(b, -1), atoms_mask.view(b, -1))\n",
    "            amino_att = self.mask_softmax(self.biatt_p[i](amino_tmp).view(b, -1), amino_mask.view(b, -1))\n",
    "\n",
    "            cf = torch.sum(atoms_vector * atoms_att.view(b, -1, 1), dim=1)\n",
    "            pf = torch.sum(amino_vector * amino_att.view(b, -1, 1), dim=1)\n",
    "\n",
    "            if i == 0:\n",
    "                cat_cf = cf\n",
    "                cat_pf = pf\n",
    "            else:\n",
    "                cat_cf = torch.cat([cat_cf.view(b, -1), cf.view(b, -1)], dim=1)\n",
    "                cat_pf = torch.cat([cat_pf.view(b, -1), pf.view(b, -1)], dim=1)\n",
    "\n",
    "        inverse_Temp = inv_Temp.view(inv_Temp.shape[0],-1)\n",
    "        Temperature = Temp.view(Temp.shape[0],-1)\n",
    "\n",
    "        # plan_1: get means\n",
    "        # cf = self.comb_c(cat_cf).view(b, -1)\n",
    "        # cf = torch.mean(cf, dim=1, keepdim=True)\n",
    "        # pf = self.comb_p(cat_pf)\n",
    "        # pf = torch.mean(pf, dim=1, keepdim=True)\n",
    "        # fps = fps.view(b, -1)\n",
    "        # fps = torch.mean(fps, dim=1, keepdim=True)\n",
    "        \n",
    "        # plan_2: only replace the last output MLP\n",
    "        # cf_final = torch.cat([self.comb_c(cat_cf).view(b, -1), fps.view(b, -1)], dim=1)#length = 2*d\n",
    "        # pf_final = self.comb_p(cat_pf)#length = d\n",
    "        # cat_vector = torch.cat((cf_final, pf_final, inverse_Temp, Temperature), dim=1)#length=3*d+2\n",
    "        # return cat_vector\n",
    "        # for j in range(self.layer_out):\n",
    "        #     cat_vector = F.leaky_relu(self.W_out[j](cat_vector), self.alpha ) \n",
    "        # return self.output(cat_vector)\n",
    "\n",
    "        #plan_3: different MLPs for different tensors\n",
    "        cf = self.comb_c(cat_cf).view(b, -1)\n",
    "        fps = fps.view(b, -1)\n",
    "        pf = self.comb_p(cat_pf)\n",
    "\n",
    "        return cf, fps, pf, inverse_Temp, Temperature\n",
    "\n",
    "    def forward(self, atoms, atoms_mask, adjacency, amino, amino_mask, fps, inv_Temp, Temp ):\n",
    "        atoms_vector = self.comp_gat(atoms, adjacency)\n",
    "        amino_vector = self.prot_cnn( amino )\n",
    "\n",
    "        super_feature = F.leaky_relu(torch.matmul(fps, self.fp0), 0.1)\n",
    "        super_feature = F.leaky_relu(torch.matmul(super_feature, self.fp1), 0.1)\n",
    "\n",
    "        prediction = self.bidirectional_attention_prediction( atoms_vector, atoms_mask, super_feature,\\\n",
    "                                                             amino_vector, amino_mask, inv_Temp, Temp )\n",
    "        \n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "class New_MLP(nn.Module):\n",
    "    def __init__(self, n_atom, n_amino, comp_dim, prot_dim, gat_dim, num_head, dropout, alpha, window, layer_cnn, latent_dim, layer_out ):\n",
    "        super(New_MLP, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.W_final_1 = nn.ModuleList([nn.Linear(latent_dim, latent_dim) for _ in range(3)])\n",
    "        self.W_final_2 = nn.ModuleList([nn.Linear(latent_dim, 1) for _ in range(3)])\n",
    "        self.W_final_3 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, cf, fps, pf, inverse_Temp, Temperature):\n",
    "        #plan_3: different MLPs for different tensors\n",
    "        cf_final = F.leaky_relu(self.W_final_2[0](F.leaky_relu(self.W_final_1[0](cf), self.alpha)), self.alpha)\n",
    "        fps_final = F.leaky_relu(self.W_final_2[1](F.leaky_relu(self.W_final_1[1](fps), self.alpha)), self.alpha)\n",
    "        pf_final = F.leaky_relu(self.W_final_2[2](F.leaky_relu(self.W_final_1[2](pf), self.alpha)), self.alpha)   \n",
    "        cat_vector = torch.cat((cf_final, fps_final, pf_final, inverse_Temp, Temperature), dim=1)\n",
    "        return self.W_final_3(cat_vector), cat_vector\n",
    "\n",
    "\n",
    "class New_MLP1(nn.Module):\n",
    "    def __init__(self, n_atom, n_amino, comp_dim, prot_dim, gat_dim, num_head, dropout, alpha, window, layer_cnn, latent_dim, layer_out ):\n",
    "        super(New_MLP1, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.W_final_1 = nn.ModuleList([nn.Linear(latent_dim*3, latent_dim*3) for _ in range(3)])\n",
    "        self.W_final_2 = nn.Linear(latent_dim*3, 3)\n",
    "        self.W_final_3 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, cf, fps, pf, inverse_Temp, Temperature):\n",
    "        #plan_3: different MLPs for different tensors\n",
    "        cat_vector = torch.cat((cf, fps, pf), dim=1)\n",
    "        for j in range(3):\n",
    "            cat_vector = F.leaky_relu(self.W_final_1[j](cat_vector), self.alpha ) \n",
    "        \n",
    "        cat_vector = F.leaky_relu(self.W_final_2(cat_vector), self.alpha ) \n",
    "        cat_vector = torch.cat((cat_vector, inverse_Temp, Temperature), dim=1)\n",
    "        return self.W_final_3(cat_vector), cat_vector\n",
    "\n",
    "def my_test(model, model_mlp, data_test, batch_size, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for i in range(math.ceil(len(data_test[0]) / batch_size)):\n",
    "        batch_data = [data_test[di][i * batch_size: (i + 1) * batch_size] for di in range(len(data_test))]\n",
    "        atoms_pad, atoms_mask, adjacencies_pad, batch_fps, amino_pad,\\\n",
    "                    amino_mask, inv_Temp, Temp, label = batch2tensor(batch_data, True, device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            cf, fps, pf, inverse_Temp, Temperature = model( atoms_pad, atoms_mask, adjacencies_pad, amino_pad, amino_mask, batch_fps, inv_Temp, Temp )\n",
    "            pred, _ = model_mlp(cf, fps, pf, inverse_Temp, Temperature)\n",
    "            \n",
    "        predictions += pred.cpu().detach().numpy().reshape(-1).tolist()\n",
    "        labels += label.cpu().numpy().reshape(-1).tolist()\n",
    "        \n",
    "    predictions = np.array(predictions)\n",
    "    labels = np.array(labels)\n",
    "    rmse, r2 = scores(labels, predictions)\n",
    "    \n",
    "    return rmse, r2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DLTKcat( len(atom_dict), len(word_dict), comp_dim, prot_dim, gat_dim, num_head, \\\n",
    "                          dropout, alpha, window, layer_cnn, latent_dim, layer_out )\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('/usr/data/DLTKcat/data/performances/model_latentdim=40_outlayer=4_rmsetest=0.8854_rmsedev=0.908.pth', map_location=device))\n",
    "\n",
    "# freeze_layers = (\"W_final_1\", \"W_final_2\", \"W_final_3\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     if name.split(\".\")[0] not in freeze_layers:\n",
    "#         param.requires_grad = False\n",
    "# model_mlp = New_MLP( len(atom_dict), len(word_dict), comp_dim, prot_dim, gat_dim, num_head, \\\n",
    "#                           dropout, alpha, window, layer_cnn, latent_dim, layer_out )\n",
    "\n",
    "def get_batch_feat(data):\n",
    "    final_feat = torch.empty((batch_size, latent_dim*3+2), device=device)\n",
    "    final_label = torch.empty((batch_size, 1), device=device)\n",
    "\n",
    "    print('data length', len(data[0]))\n",
    "    for i in range(math.ceil( len(data[0]) / batch_size )):\n",
    "        batch_data = [data[di][ i* batch_size: (i + 1) * batch_size] \\\n",
    "                            for di in range(len(data))]\n",
    "        atoms_pad, atoms_mask, adjacencies_pad, batch_fps, amino_pad,\\\n",
    "                                amino_mask, inv_Temp, Temp, label = batch2tensor(batch_data, True, device)\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            cat_vector = model( atoms_pad, atoms_mask, adjacencies_pad, amino_pad, amino_mask, batch_fps, inv_Temp, Temp )\n",
    "\n",
    "        final_feat = torch.cat((final_feat, cat_vector), dim=0)\n",
    "        final_label = torch.cat((final_label, label), dim=0)\n",
    "    return final_feat, final_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length 17532\n",
      "data length 2164\n"
     ]
    }
   ],
   "source": [
    "temp_dataset = {}\n",
    "temp_dataset['train_input'], temp_dataset['train_label'] = get_batch_feat(train_data)\n",
    "temp_dataset['test_input'], temp_dataset['test_label'] = get_batch_feat(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17540, 122])\n",
      "torch.Size([17540, 1])\n",
      "torch.Size([2172, 122])\n",
      "torch.Size([2172, 1])\n"
     ]
    }
   ],
   "source": [
    "for key, values in temp_dataset.items():\n",
    "    print(values.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 6.66e-01 | test loss: 1.01e+00 | reg: 2.71e+00 : 100%|████| 2/2 [00:05<00:00,  2.89s/it]\n"
     ]
    }
   ],
   "source": [
    "from kan import *\n",
    "\n",
    "criterion = F.mse_loss\n",
    "model_kan = KAN(width=[5, 1], grid=5, k=3, seed=args.seed, device=device)\n",
    "print(model_kan.device)\n",
    "results = model_kan.train(temp_dataset, opt=\"LBFGS\", steps=2, lamb=0.01, loss_fn=criterion, device=device, lamb_entropy=2.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 33701\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model_kan.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6658134 1.0083215\n",
      "17540\n",
      "17540\n",
      "2172\n",
      "2172\n",
      "rmse_train 0.665813 r2_train 0.772156\n",
      "rmse_test 1.008322 r2_test 0.421454\n"
     ]
    }
   ],
   "source": [
    "print(results['train_loss'][-1], results['test_loss'][-1])\n",
    "print(len(results['train_pred']))\n",
    "print(len(results['train_labels']))\n",
    "print(len(results['test_pred']))\n",
    "print(len(results['test_labels']))\n",
    "rmse_train, r2_train = scores(np.array(results['train_pred'][-17540:]), np.array(results['train_labels'][-17540:]))\n",
    "print('rmse_train', rmse_train, 'r2_train', r2_train)\n",
    "rmse_test, r2_test = scores(np.array(results['test_pred'][-2172:]), np.array(results['test_labels'][-2172:]))\n",
    "print('rmse_test', rmse_test, 'r2_test', r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing (0,0,0) with abs, r2=0.9994272589683533\n",
      "fixing (0,1,0) with exp, r2=0.4502962827682495\n",
      "fixing (0,2,0) with exp, r2=0.9999526739120483\n",
      "fixing (0,3,0) with abs, r2=0.8135745525360107\n",
      "fixing (0,4,0) with exp, r2=0.4711735248565674\n",
      "0.01*exp(3.13*x_2) - 0.04*Abs(8.17*x_1 - 0.42) - 0.07*Abs(4.54*x_4 - 1.78) + 0.21973 - 0.06*exp(-5.36*x_5)\n"
     ]
    }
   ],
   "source": [
    "# model_kan.plot()\n",
    "lib = ['x','x^2','exp','log','sqrt','abs']\n",
    "model_kan.auto_symbolic(lib=lib)\n",
    "formula = model_kan.symbolic_formula()\n",
    "print(formula[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_kan.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_kan.save_ckpt('kan_lastmlp.ckpt')\n",
    "# model_kan = KAN(width=[122, 10, 10, 1], grid=5, k=3, seed=args.seed, device=device)\n",
    "# model_kan.load_ckpt('kan_lastmlp.ckpt')\n",
    "# model_kan.train(temp_dataset, opt=\"LBFGS\", steps=1, lamb=0.01, loss_fn=criterion, device=device)\n",
    "# model_kan.plot()\n",
    "\n",
    "# obtaining symbolic formula\n",
    "# formula, variables = model_kan.symbolic_formula()\n",
    "# model_kan.suggest_symbolic(2,0,0)\n",
    "# print(model_kan.symbolic_formula())\n",
    "# formula[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval_my(model, model_mlp, data_train, data_test, data_dev, device, lr, batch_size, lr_decay, decay_interval, num_epochs ):\n",
    "    criterion = F.mse_loss\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0, amsgrad=True)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size= decay_interval, gamma=lr_decay)\n",
    "    idx = np.arange(len(data_train[0]))\n",
    "    \n",
    "    min_size = 4   #4\n",
    "    if batch_size > min_size:\n",
    "        div_min = int(batch_size / min_size)\n",
    "    else:\n",
    "        div_min = 1\n",
    "        \n",
    "    rmse_train_scores, r2_train_scores, rmse_test_scores, r2_test_scores, rmse_dev_scores, r2_dev_scores = [],[],[],[],[],[]\n",
    "    for epoch in range(num_epochs):\n",
    "             \n",
    "        np.random.shuffle(idx)\n",
    "        model.train()\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        for i in range(math.ceil( len(data_train[0]) / min_size )):\n",
    "            batch_data = [data_train[di][idx[ i* min_size: (i + 1) * min_size]] \\\n",
    "                          for di in range(len(data_train))]\n",
    "            atoms_pad, atoms_mask, adjacencies_pad, batch_fps, amino_pad,\\\n",
    "                            amino_mask, inv_Temp, Temp, label = batch2tensor(batch_data, True, device)\n",
    "            \n",
    "            cf, fps, pf, inverse_Temp, Temperature = model( atoms_pad, atoms_mask, adjacencies_pad, amino_pad, amino_mask, batch_fps, inv_Temp, Temp )\n",
    "            pred, _ = model_mlp(cf, fps, pf, inverse_Temp, Temperature)\n",
    "            loss = criterion(pred.float(), label.float())\n",
    "            predictions += pred.cpu().detach().numpy().reshape(-1).tolist()\n",
    "            labels += label.cpu().numpy().reshape(-1).tolist()\n",
    "            loss.backward()\n",
    "            if i % div_min == 0 and i != 0:    \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        labels = np.array(labels)\n",
    "        rmse_train, r2_train = scores( labels, predictions )\n",
    "        rmse_dev, r2_dev = my_test( model, model_mlp, data_dev, batch_size, device ) #dev dataset\n",
    "        rmse_test, r2_test = my_test(model, model_mlp, data_test, batch_size, device) # test dataset\n",
    "        \n",
    "        if rmse_test < 0.91:\n",
    "            print('Best model found at epoch=' + str(epoch) + '!')\n",
    "            best_model_pth = '/usr/data/DLTKcat/data/my_performance/model_latentdim=' + str(model.latent_dim) + '_outlayer=' + str(model.layer_out)\n",
    "            best_model_pth = best_model_pth + '_rmsetest='+str( round(rmse_test,4) )+'_rmsedev='+str( round(rmse_dev,4) ) +'.pth'\n",
    "            torch.save( model.state_dict(), best_model_pth)\n",
    "\n",
    "\n",
    "        rmse_train_scores.append( rmse_train )\n",
    "        r2_train_scores.append( r2_train )\n",
    "        rmse_dev_scores.append( rmse_dev )\n",
    "        r2_dev_scores.append( r2_dev )\n",
    "        rmse_test_scores.append( rmse_test )\n",
    "        r2_test_scores.append( r2_test )\n",
    "\n",
    "        \n",
    "        print('epoch: '+str(epoch)+'/'+ str(num_epochs) +';  rmse test: ' + str(rmse_test) + '; r2 test: ' + str(r2_test) )\n",
    "        \n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "    return rmse_train_scores, r2_train_scores, rmse_test_scores, r2_test_scores, rmse_dev_scores, r2_dev_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/10;  rmse test: 1.125471; r2 test: 0.459208\n",
      "epoch: 1/10;  rmse test: 1.030305; r2 test: 0.546796\n",
      "epoch: 2/10;  rmse test: 1.011667; r2 test: 0.563045\n",
      "epoch: 3/10;  rmse test: 0.980582; r2 test: 0.589485\n",
      "epoch: 4/10;  rmse test: 0.958266; r2 test: 0.607957\n",
      "epoch: 5/10;  rmse test: 0.965739; r2 test: 0.601819\n",
      "epoch: 6/10;  rmse test: 0.946624; r2 test: 0.617425\n",
      "epoch: 7/10;  rmse test: 0.957213; r2 test: 0.608818\n",
      "epoch: 8/10;  rmse test: 0.946306; r2 test: 0.617682\n",
      "epoch: 9/10;  rmse test: 0.956318; r2 test: 0.609549\n"
     ]
    }
   ],
   "source": [
    "model = DLTKcat( len(atom_dict), len(word_dict), comp_dim, prot_dim, gat_dim, num_head, \\\n",
    "                          dropout, alpha, window, layer_cnn, latent_dim, layer_out )\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('/usr/data/DLTKcat/data/performances/model_latentdim=40_outlayer=4_rmsetest=0.8854_rmsedev=0.908.pth', map_location=device))\n",
    "\n",
    "# freeze_layers = (\"W_final_1\", \"W_final_2\", \"W_final_3\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     if name.split(\".\")[0] not in freeze_layers:\n",
    "        # param.requires_grad = False\n",
    "model_mlp = New_MLP1( len(atom_dict), len(word_dict), comp_dim, prot_dim, gat_dim, num_head, \\\n",
    "                          dropout, alpha, window, layer_cnn, latent_dim, layer_out )\n",
    "model_mlp.to(device)\n",
    "rmse_train_scores, r2_train_scores, rmse_test_scores, r2_test_scores ,rmse_dev_scores, r2_dev_scores = \\\n",
    "        train_eval_my(model, model_mlp , train_data, test_data, dev_data, device, lr, batch_size, lr_decay,\\\n",
    "                   decay_interval, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), '/usr/data/DLTKcat/data/my_performance/DLTKcat_finetune.pth')\n",
    "# torch.save(model_mlp.state_dict(), '/usr/data/DLTKcat/data/my_performance/mlp.pth')\n",
    "\n",
    "model = DLTKcat( len(atom_dict), len(word_dict), comp_dim, prot_dim, gat_dim, num_head, \\\n",
    "                          dropout, alpha, window, layer_cnn, latent_dim, layer_out )\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('/usr/data/DLTKcat/data/my_performance/DLTKcat_finetune.pth', map_location=device))\n",
    "\n",
    "model_mlp = New_MLP1( len(atom_dict), len(word_dict), comp_dim, prot_dim, gat_dim, num_head, \\\n",
    "                          dropout, alpha, window, layer_cnn, latent_dim, layer_out )\n",
    "model_mlp.to(device)\n",
    "model_mlp.load_state_dict(torch.load('/usr/data/DLTKcat/data/my_performance/mlp.pth', map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length 17532\n",
      "data length 2164\n",
      "torch.Size([17540, 5])\n",
      "torch.Size([17540, 1])\n",
      "torch.Size([2172, 5])\n",
      "torch.Size([2172, 1])\n"
     ]
    }
   ],
   "source": [
    "def get_batch_feat(data, model, model_mlp):\n",
    "    final_feat = torch.empty((batch_size, 5), device=device)\n",
    "    final_label = torch.empty((batch_size, 1), device=device)\n",
    "\n",
    "    print('data length', len(data[0]))\n",
    "    for i in range(math.ceil( len(data[0]) / batch_size )):\n",
    "        batch_data = [data[di][ i* batch_size: (i + 1) * batch_size] \\\n",
    "                            for di in range(len(data))]\n",
    "        atoms_pad, atoms_mask, adjacencies_pad, batch_fps, amino_pad,\\\n",
    "                                amino_mask, inv_Temp, Temp, label = batch2tensor(batch_data, True, device)\n",
    "        with torch.no_grad():\n",
    "            # with torch.no_grad():\n",
    "            cf, fps, pf, inverse_Temp, Temperature = model( atoms_pad, atoms_mask, adjacencies_pad, amino_pad, amino_mask, batch_fps, inv_Temp, Temp )\n",
    "            pred, cat_vector = model_mlp(cf, fps, pf, inverse_Temp, Temperature)\n",
    "            # cat_vector = model( atoms_pad, atoms_mask, adjacencies_pad, amino_pad, amino_mask, batch_fps, inv_Temp, Temp )\n",
    "\n",
    "        final_feat = torch.cat((final_feat, cat_vector), dim=0)\n",
    "        final_label = torch.cat((final_label, label), dim=0)\n",
    "    return final_feat, final_label\n",
    "\n",
    "temp_dataset = {}\n",
    "temp_dataset['train_input'], temp_dataset['train_label'] = get_batch_feat(train_data, model, model_mlp)\n",
    "temp_dataset['test_input'], temp_dataset['test_label'] = get_batch_feat(test_data, model, model_mlp)\n",
    "for key, values in temp_dataset.items():\n",
    "    print(values.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Uni_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
