{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Uni_test/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data from ../data/ .\n",
      "Loading test data from ../data/ .\n",
      "Loading parameters from ../data/hyparams/param_2.pkl .\n",
      "Namespace(batch=8, decay_interval=10, lr=0.001, lr_decay=0.5, num_epoch=1, param_dict_pkl='../data/hyparams/param_2.pkl', seed=42, shuffle_T='False', test_path='../data/', train_path='../data/')\n",
      "{'comp_dim': 80, 'prot_dim': 80, 'gat_dim': 50, 'num_head': 3, 'dropout': 0.1, 'alpha': 0.1, 'window': 5, 'layer_cnn': 4, 'latent_dim': 40, 'layer_out': 4}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from train_functions import *\n",
    "from feature_functions import load_pickle, dump_pickle\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Inputs: --train_path: train data path; --test_path: test data path;\\\n",
    "                        --lr: learning rate;--batch: batch size; --lr_decay:multiplicative factor of learning rate decay. Default: 0.5;\\\n",
    "                        --decay_interval: period of learning rate decay; --num_epoch: the number of epochs;\\\n",
    "                        --param_dict_pkl: the path to parameters; --shuffle_T: shuffle temperature feature if True. Default=False')\n",
    "    \n",
    "parser.add_argument('--train_path', default='../data/')\n",
    "parser.add_argument('--test_path', default='../data/')\n",
    "parser.add_argument('--lr', default = 0.001, type=float )\n",
    "parser.add_argument('--batch', default = 8 , type=int )\n",
    "parser.add_argument('--lr_decay', default = 0.5, type=float )\n",
    "parser.add_argument('--decay_interval', default = 10, type=int )\n",
    "parser.add_argument('--num_epoch', default = 1, type=int )\n",
    "parser.add_argument('--seed', default = 42, type=int )\n",
    "parser.add_argument('--param_dict_pkl', default = '../data/hyparams/param_2.pkl')\n",
    "parser.add_argument('--shuffle_T', default = 'False', choices=['False','True'], type = str )\n",
    "# args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "\n",
    "train_path, test_path, lr, batch_size, lr_decay, decay_interval, param_dict_pkl = \\\n",
    "        str(args.train_path), str(args.test_path), float(args.lr), int(args.batch), \\\n",
    "        float(args.lr_decay), int(args.decay_interval) , str( args.param_dict_pkl )\n",
    "\n",
    "print('Loading train data from %s .' % train_path)\n",
    "if not ( os.path.isdir(train_path) ):\n",
    "    raise SystemExit('Directory %s does not exist!' % train_path )\n",
    "    \n",
    "print('Loading test data from %s .' % test_path)\n",
    "if not ( os.path.isdir(test_path) ):\n",
    "    raise SystemExit('Directory %s does not exist!' % test_path )\n",
    "    \n",
    "print('Loading parameters from %s .' % param_dict_pkl)\n",
    "if not ( os.path.exists(param_dict_pkl) ):\n",
    "    raise SystemExit('File %s does not exist!' % param_dict_pkl )\n",
    "    \n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "print(args)\n",
    "\n",
    "param_dict = load_pickle(param_dict_pkl)\n",
    "comp_dim, prot_dim, gat_dim, num_head, dropout, alpha, window, layer_cnn, latent_dim, layer_out = \\\n",
    "        param_dict['comp_dim'], param_dict['prot_dim'],param_dict['gat_dim'],param_dict['num_head'],\\\n",
    "        param_dict['dropout'], param_dict['alpha'], param_dict['window'], param_dict['layer_cnn'], \\\n",
    "        param_dict['latent_dim'], param_dict['layer_out']\n",
    "\n",
    "print(param_dict)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Setting attributes on ParameterList is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_dict = load_pickle(  '../data/dict/fingerprint_dict.pkl' )\n",
    "word_dict = load_pickle(   '../data/dict/word_dict.pkl' )\n",
    "\n",
    "datapack = load_data(train_path, True, 'train')\n",
    "test_data = load_data(test_path, True, 'test')\n",
    "\n",
    "train_data, dev_data = split_data( datapack, 0.1 )\n",
    "\n",
    "num_epochs = int( args.num_epoch )#fixed value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout=0.5, alpha=0.2, concat=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.matmul(h, self.W)\n",
    "        a_input = self._prepare_attentional_mechanism_input(Wh)\n",
    "        e = F.leaky_relu(torch.matmul(a_input, self.a).squeeze(3), self.alpha)\n",
    "\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=2)\n",
    "        # attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.bmm(attention, Wh)\n",
    "\n",
    "        return F.elu(h_prime) if self.concat else h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        b = Wh.size()[0]\n",
    "        N = Wh.size()[1]\n",
    "\n",
    "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=1)\n",
    "        Wh_repeated_alternating = Wh.repeat_interleave(N, dim=0).view(b, N*N, self.out_features)\n",
    "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=2)\n",
    "\n",
    "        return all_combinations_matrix.view(b, N, N, 2 * self.out_features)\n",
    "\n",
    "\n",
    "class KcatModel(nn.Module):\n",
    "    def __init__(self, n_atom, n_amino, comp_dim, prot_dim, gat_dim, num_head, dropout, alpha, window, layer_cnn, latent_dim, layer_out ):\n",
    "        super(KcatModel, self).__init__()\n",
    "        '''\n",
    "        n_atom here stands for number of atom_features\n",
    "        '''\n",
    "\n",
    "\n",
    "        self.embedding_layer_atom = nn.Embedding(n_atom+1, comp_dim)\n",
    "        self.embedding_layer_amino = nn.Embedding(n_amino+1, prot_dim)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.layer_cnn = layer_cnn\n",
    "        self.latent_dim = latent_dim\n",
    "        self.layer_out = layer_out\n",
    "\n",
    "        self.gat_layers = [GATLayer(comp_dim, gat_dim, dropout=dropout, alpha=alpha, concat=True)\n",
    "                           for _ in range(num_head)]\n",
    "        for i, layer in enumerate(self.gat_layers):\n",
    "            self.add_module('gat_layer_{}'.format(i), layer)\n",
    "        self.gat_out = GATLayer(gat_dim * num_head, comp_dim, dropout=dropout, alpha=alpha, concat=False)\n",
    "        self.W_comp = nn.Linear(comp_dim, latent_dim)\n",
    "\n",
    "        self.conv_layers = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2*window+1,\n",
    "                                                    stride=1, padding=window) for _ in range(layer_cnn)])\n",
    "        self.W_prot = nn.Linear(prot_dim, latent_dim)\n",
    "\n",
    "        self.fp0 = nn.Parameter(torch.empty(size=(1024, latent_dim)))\n",
    "        nn.init.xavier_uniform_(self.fp0, gain=1.414)\n",
    "        self.fp1 = nn.Parameter(torch.empty(size=(latent_dim, latent_dim)))\n",
    "        nn.init.xavier_uniform_(self.fp1, gain=1.414)\n",
    "\n",
    "        self.bidat_num = 4\n",
    "\n",
    "        self.U = nn.ParameterList([nn.Parameter(torch.empty(size=(latent_dim, latent_dim))) for _ in range(self.bidat_num)])\n",
    "        for i in range(self.bidat_num):\n",
    "            nn.init.xavier_uniform_(self.U[i], gain=1.414)\n",
    "\n",
    "        self.transform_c2p = nn.ModuleList([nn.Linear(latent_dim, latent_dim) for _ in range(self.bidat_num)])\n",
    "        self.transform_p2c = nn.ModuleList([nn.Linear(latent_dim, latent_dim) for _ in range(self.bidat_num)])\n",
    "\n",
    "        self.bihidden_c = nn.ModuleList([nn.Linear(latent_dim, latent_dim) for _ in range(self.bidat_num)])\n",
    "        self.bihidden_p = nn.ModuleList([nn.Linear(latent_dim, latent_dim) for _ in range(self.bidat_num)])\n",
    "        self.biatt_c = nn.ModuleList([nn.Linear(latent_dim * 2, 1) for _ in range(self.bidat_num)])\n",
    "        self.biatt_p = nn.ModuleList([nn.Linear(latent_dim * 2, 1) for _ in range(self.bidat_num)])\n",
    "\n",
    "        self.comb_c = nn.Linear(latent_dim * self.bidat_num, latent_dim)\n",
    "        self.comb_p = nn.Linear(latent_dim * self.bidat_num, latent_dim)\n",
    "   \n",
    "        self.W_out = nn.ModuleList([nn.Linear(latent_dim * 3 + 2, latent_dim * 3 + 2)\n",
    "                                    for _ in range(self.layer_out)])\n",
    "    \n",
    "        self.output = nn.Linear(latent_dim * 3 + 2, 1)\n",
    "\n",
    "\n",
    "    def comp_gat(self, atoms, adj):\n",
    "        atoms_vector = self.embedding_layer_atom(atoms)\n",
    "        atoms_multi_head = torch.cat([gat(atoms_vector, adj) for gat in self.gat_layers], dim=2)\n",
    "        atoms_vector = F.elu(self.gat_out(atoms_multi_head, adj))\n",
    "        atoms_vector = F.leaky_relu(self.W_comp(atoms_vector), self.alpha)\n",
    "        return atoms_vector #(B,L, 40)\n",
    "\n",
    "    def prot_cnn(self, amino ):\n",
    "        amino_vector = self.embedding_layer_amino(amino)\n",
    "        amino_vector = torch.unsqueeze(amino_vector, 1)\n",
    "        for i in range(self.layer_cnn):\n",
    "            amino_vector = F.leaky_relu(self.conv_layers[i](amino_vector), self.alpha)\n",
    "        amino_vector = torch.squeeze(amino_vector, 1)\n",
    "        amino_vector = F.leaky_relu(self.W_prot(amino_vector), self.alpha)\n",
    "        return amino_vector\n",
    "\n",
    "    def mask_softmax(self, a, mask, dim=-1):\n",
    "        a_max = torch.max(a, dim, keepdim=True)[0]\n",
    "        a_exp = torch.exp(a - a_max)\n",
    "        a_exp = a_exp * mask\n",
    "        a_softmax = a_exp / (torch.sum(a_exp, dim, keepdim=True) + 1e-6)\n",
    "        return a_softmax\n",
    "\n",
    "    def bidirectional_attention_prediction(self,atoms_vector, atoms_mask, fps, amino_vector, amino_mask, inv_Temp, Temp):\n",
    "        b = atoms_vector.shape[0]\n",
    "        for i in range(self.bidat_num):\n",
    "            A = torch.tanh(torch.matmul(torch.matmul(atoms_vector, self.U[i]), amino_vector.transpose(1, 2)))\n",
    "            A = A * torch.matmul(atoms_mask.view(b, -1, 1), amino_mask.view(b, 1, -1))\n",
    "\n",
    "            atoms_trans = torch.matmul(A, torch.tanh(self.transform_p2c[i](amino_vector)))\n",
    "            amino_trans = torch.matmul(A.transpose(1, 2), torch.tanh(self.transform_c2p[i](atoms_vector)))\n",
    "\n",
    "            atoms_tmp = torch.cat([torch.tanh(self.bihidden_c[i](atoms_vector)), atoms_trans], dim=2)\n",
    "            amino_tmp = torch.cat([torch.tanh(self.bihidden_p[i](amino_vector)), amino_trans], dim=2)\n",
    "\n",
    "            atoms_att = self.mask_softmax(self.biatt_c[i](atoms_tmp).view(b, -1), atoms_mask.view(b, -1))\n",
    "            amino_att = self.mask_softmax(self.biatt_p[i](amino_tmp).view(b, -1), amino_mask.view(b, -1))\n",
    "\n",
    "            cf = torch.sum(atoms_vector * atoms_att.view(b, -1, 1), dim=1)\n",
    "            pf = torch.sum(amino_vector * amino_att.view(b, -1, 1), dim=1)\n",
    "\n",
    "            if i == 0:\n",
    "                cat_cf = cf\n",
    "                cat_pf = pf\n",
    "            else:\n",
    "                cat_cf = torch.cat([cat_cf.view(b, -1), cf.view(b, -1)], dim=1)\n",
    "                cat_pf = torch.cat([cat_pf.view(b, -1), pf.view(b, -1)], dim=1)\n",
    "\n",
    "        inverse_Temp = inv_Temp.view(inv_Temp.shape[0],-1)\n",
    "        Temperature = Temp.view(Temp.shape[0],-1)\n",
    "\n",
    "        # plan_1: get means\n",
    "        # cf = self.comb_c(cat_cf).view(b, -1)\n",
    "        # cf = torch.mean(cf, dim=1, keepdim=True)\n",
    "        # pf = self.comb_p(cat_pf)\n",
    "        # pf = torch.mean(pf, dim=1, keepdim=True)\n",
    "        # fps = fps.view(b, -1)\n",
    "        # fps = torch.mean(fps, dim=1, keepdim=True)\n",
    "        \n",
    "        # plan_2: only replace the last output MLP\n",
    "        # cf_final = torch.cat([self.comb_c(cat_cf).view(b, -1), fps.view(b, -1)], dim=1)#length = 2*d\n",
    "        # pf_final = self.comb_p(cat_pf)#length = d\n",
    "        # cat_vector = torch.cat((cf_final, pf_final, inverse_Temp, Temperature), dim=1)#length=3*d+2\n",
    "        # return cat_vector\n",
    "        # for j in range(self.layer_out):\n",
    "        #     cat_vector = F.leaky_relu(self.W_out[j](cat_vector), self.alpha ) \n",
    "        # return self.output(cat_vector)\n",
    "\n",
    "        #plan_3: different MLPs for different tensors\n",
    "        cf = self.comb_c(cat_cf).view(b, -1)\n",
    "        fps = fps.view(b, -1)\n",
    "        pf = self.comb_p(cat_pf)\n",
    "\n",
    "        return cf, fps, pf, inverse_Temp, Temperature\n",
    "\n",
    "    def forward(self, atoms, atoms_mask, adjacency, amino, amino_mask, fps, inv_Temp, Temp ):\n",
    "        atoms_vector = self.comp_gat(atoms, adjacency)\n",
    "        amino_vector = self.prot_cnn( amino )\n",
    "\n",
    "\n",
    "        super_feature = F.leaky_relu(torch.matmul(fps, self.fp0), 0.1)\n",
    "        super_feature = F.leaky_relu(torch.matmul(super_feature, self.fp1), 0.1)\n",
    "\n",
    "        prediction = self.bidirectional_attention_prediction( atoms_vector, atoms_mask, super_feature,\\\n",
    "                                                             amino_vector, amino_mask, inv_Temp, Temp )\n",
    "        \n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "class New_MLP(nn.Module):\n",
    "    def __init__(self, n_atom, n_amino, comp_dim, prot_dim, gat_dim, num_head, dropout, alpha, window, layer_cnn, latent_dim, layer_out ):\n",
    "        super(New_MLP, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.W_final_1 = nn.ModuleList([nn.Linear(latent_dim, latent_dim) for _ in range(3)])\n",
    "        self.W_final_2 = nn.ModuleList([nn.Linear(latent_dim, 1) for _ in range(3)])\n",
    "        self.W_final_3 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, cf, fps, pf, inverse_Temp, Temperature):\n",
    "        #plan_3: same MLPs for different tensors\n",
    "        cf_final = F.leaky_relu(self.W_final_2[0](F.leaky_relu(self.W_final_1[0](cf), self.alpha)), self.alpha)\n",
    "        fps_final = F.leaky_relu(self.W_final_2[1](F.leaky_relu(self.W_final_1[1](fps), self.alpha)), self.alpha)\n",
    "        pf_final = F.leaky_relu(self.W_final_2[2](F.leaky_relu(self.W_final_1[2](pf), self.alpha)), self.alpha)   \n",
    "        cat_vector = torch.cat((cf_final, fps_final, pf_final, inverse_Temp, Temperature), dim=1)\n",
    "        return self.W_final_3(cat_vector), cat_vector\n",
    "\n",
    "\n",
    "class New_MLP_Independent(nn.Module):\n",
    "    def __init__(self, n_atom, n_amino, comp_dim, prot_dim, gat_dim, num_head, dropout, alpha, window, layer_cnn, latent_dim, layer_out ):\n",
    "        super(New_MLP_Independent, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.W_final_1 = nn.ModuleList([nn.Linear(latent_dim*3, latent_dim*3) for _ in range(3)])\n",
    "        self.W_final_2 = nn.Linear(latent_dim*3, 3)\n",
    "        self.W_final_3 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, cf, fps, pf, inverse_Temp, Temperature):\n",
    "        #plan_3: different MLPs for different tensors\n",
    "        cat_vector = torch.cat((cf, fps, pf), dim=1)\n",
    "        for j in range(3):\n",
    "            cat_vector = F.leaky_relu(self.W_final_1[j](cat_vector), self.alpha ) \n",
    "        \n",
    "        cat_vector = F.leaky_relu(self.W_final_2(cat_vector), self.alpha ) \n",
    "        cat_vector = torch.cat((cat_vector, inverse_Temp, Temperature), dim=1)\n",
    "        return self.W_final_3(cat_vector), cat_vector\n",
    "\n",
    "def test(model, model_mlp, data_test, batch_size, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for i in range(math.ceil(len(data_test[0]) / batch_size)):\n",
    "        batch_data = [data_test[di][i * batch_size: (i + 1) * batch_size] for di in range(len(data_test))]\n",
    "        atoms_pad, atoms_mask, adjacencies_pad, batch_fps, amino_pad,\\\n",
    "                    amino_mask, inv_Temp, Temp, label = DLTKcat_batch2tensor(batch_data, True, device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            cf, fps, pf, inverse_Temp, Temperature = model( atoms_pad, atoms_mask, adjacencies_pad, amino_pad, amino_mask, batch_fps, inv_Temp, Temp )\n",
    "            pred, _ = model_mlp(cf, fps, pf, inverse_Temp, Temperature)\n",
    "            \n",
    "        predictions += pred.cpu().detach().numpy().reshape(-1).tolist()\n",
    "        labels += label.cpu().numpy().reshape(-1).tolist()\n",
    "        \n",
    "    predictions = np.array(predictions)\n",
    "    labels = np.array(labels)\n",
    "    rmse, r2 = scores(labels, predictions)\n",
    "    \n",
    "    return rmse, r2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_eval(model, model_mlp, data_train, data_test, data_dev, device, lr, batch_size, lr_decay, decay_interval, num_epochs ):\n",
    "    criterion = F.mse_loss\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0, amsgrad=True)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size= decay_interval, gamma=lr_decay)\n",
    "    idx = np.arange(len(data_train[0]))\n",
    "    \n",
    "    min_size = 4   #4\n",
    "    if batch_size > min_size:\n",
    "        div_min = int(batch_size / min_size)\n",
    "    else:\n",
    "        div_min = 1\n",
    "        \n",
    "    rmse_train_scores, r2_train_scores, rmse_test_scores, r2_test_scores, rmse_dev_scores, r2_dev_scores = [],[],[],[],[],[]\n",
    "    for epoch in range(num_epochs):\n",
    "             \n",
    "        np.random.shuffle(idx)\n",
    "        model.train()\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        for i in range(math.ceil( len(data_train[0]) / min_size )):\n",
    "            batch_data = [data_train[di][idx[ i* min_size: (i + 1) * min_size]] \\\n",
    "                          for di in range(len(data_train))]\n",
    "            atoms_pad, atoms_mask, adjacencies_pad, batch_fps, amino_pad,\\\n",
    "                            amino_mask, inv_Temp, Temp, label = DLTKcat_batch2tensor(batch_data, True, device)\n",
    "            \n",
    "            cf, fps, pf, inverse_Temp, Temperature = model( atoms_pad, atoms_mask, adjacencies_pad, amino_pad, amino_mask, batch_fps, inv_Temp, Temp )\n",
    "            pred, _ = model_mlp(cf, fps, pf, inverse_Temp, Temperature)\n",
    "            loss = criterion(pred.float(), label.float())\n",
    "            predictions += pred.cpu().detach().numpy().reshape(-1).tolist()\n",
    "            labels += label.cpu().numpy().reshape(-1).tolist()\n",
    "            loss.backward()\n",
    "            if i % div_min == 0 and i != 0:    \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        labels = np.array(labels)\n",
    "        rmse_train, r2_train = scores( labels, predictions )\n",
    "        rmse_dev, r2_dev = test( model, model_mlp, data_dev, batch_size, device ) #dev dataset\n",
    "        rmse_test, r2_test = test(model, model_mlp, data_test, batch_size, device) # test dataset\n",
    "        \n",
    "        if rmse_test < 0.91:\n",
    "            print('Best model found at epoch=' + str(epoch) + '!')\n",
    "            best_model_pth = '../data/my_performance/model_latentdim=' + str(model.latent_dim) + '_outlayer=' + str(model.layer_out)\n",
    "            best_model_pth = best_model_pth + '_rmsetest='+str( round(rmse_test,4) )+'_rmsedev='+str( round(rmse_dev,4) ) +'.pth'\n",
    "            torch.save( model.state_dict(), best_model_pth)\n",
    "\n",
    "\n",
    "        rmse_train_scores.append( rmse_train )\n",
    "        r2_train_scores.append( r2_train )\n",
    "        rmse_dev_scores.append( rmse_dev )\n",
    "        r2_dev_scores.append( r2_dev )\n",
    "        rmse_test_scores.append( rmse_test )\n",
    "        r2_test_scores.append( r2_test )\n",
    "\n",
    "        \n",
    "        print('epoch: '+str(epoch)+'/'+ str(num_epochs) +';  rmse test: ' + str(rmse_test) + '; r2 test: ' + str(r2_test) )\n",
    "        \n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "    return rmse_train_scores, r2_train_scores, rmse_test_scores, r2_test_scores, rmse_dev_scores, r2_dev_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KcatModel( len(atom_dict), len(word_dict), comp_dim, prot_dim, gat_dim, num_head, \\\n",
    "                          dropout, alpha, window, layer_cnn, latent_dim, layer_out )\n",
    "model.to(device)\n",
    "# model.load_state_dict(torch.load('../data/performances/model_latentdim=40_outlayer=4_rmsetest=0.8854_rmsedev=0.908.pth', map_location=device))\n",
    "\n",
    "model_mlp = New_MLP_Independent( len(atom_dict), len(word_dict), comp_dim, prot_dim, gat_dim, num_head, \\\n",
    "                          dropout, alpha, window, layer_cnn, latent_dim, layer_out )\n",
    "model_mlp.to(device)\n",
    "rmse_train_scores, r2_train_scores, rmse_test_scores, r2_test_scores ,rmse_dev_scores, r2_dev_scores = \\\n",
    "        train_eval(model, model_mlp , train_data, test_data, dev_data, device, lr, batch_size, lr_decay,\\\n",
    "                   decay_interval, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), '../data/my_performance/DLTKcat_finetune.pth')\n",
    "# torch.save(model_mlp.state_dict(), '../data/my_performance/mlp.pth')\n",
    "\n",
    "model = KcatModel( len(atom_dict), len(word_dict), comp_dim, prot_dim, gat_dim, num_head, \\\n",
    "                          dropout, alpha, window, layer_cnn, latent_dim, layer_out )\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('../data/my_performance/DLTKcat_finetune.pth', map_location=device))\n",
    "\n",
    "model_mlp = New_MLP_Independent( len(atom_dict), len(word_dict), comp_dim, prot_dim, gat_dim, num_head, \\\n",
    "                          dropout, alpha, window, layer_cnn, latent_dim, layer_out )\n",
    "model_mlp.to(device)\n",
    "model_mlp.load_state_dict(torch.load('../data/my_performance/mlp.pth', map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length 17532\n",
      "data length 2164\n",
      "train_input torch.Size([17540, 5])\n",
      "train_label torch.Size([17540, 1])\n",
      "test_input torch.Size([2172, 5])\n",
      "test_label torch.Size([2172, 1])\n"
     ]
    }
   ],
   "source": [
    "def get_batch_feat(data, model, model_mlp):\n",
    "    final_feat = torch.empty((batch_size, 5), device=device)\n",
    "    final_label = torch.empty((batch_size, 1), device=device)\n",
    "\n",
    "    print('data length', len(data[0]))\n",
    "    for i in range(math.ceil( len(data[0]) / batch_size )):\n",
    "        batch_data = [data[di][ i* batch_size: (i + 1) * batch_size] \\\n",
    "                            for di in range(len(data))]\n",
    "        atoms_pad, atoms_mask, adjacencies_pad, batch_fps, amino_pad,\\\n",
    "                                amino_mask, inv_Temp, Temp, label = DLTKcat_batch2tensor(batch_data, True, device)\n",
    "        with torch.no_grad():\n",
    "            # with torch.no_grad():\n",
    "            cf, fps, pf, inverse_Temp, Temperature = model( atoms_pad, atoms_mask, adjacencies_pad, amino_pad, amino_mask, batch_fps, inv_Temp, Temp )\n",
    "            pred, cat_vector = model_mlp(cf, fps, pf, inverse_Temp, Temperature)\n",
    "            # cat_vector = model( atoms_pad, atoms_mask, adjacencies_pad, amino_pad, amino_mask, batch_fps, inv_Temp, Temp )\n",
    "\n",
    "        final_feat = torch.cat((final_feat, cat_vector), dim=0)\n",
    "        final_label = torch.cat((final_label, label), dim=0)\n",
    "    return final_feat, final_label\n",
    "\n",
    "temp_dataset = {}\n",
    "temp_dataset['train_input'], temp_dataset['train_label'] = get_batch_feat(train_data, model, model_mlp)\n",
    "temp_dataset['test_input'], temp_dataset['test_label'] = get_batch_feat(test_data, model, model_mlp)\n",
    "for key, values in temp_dataset.items():\n",
    "    print(key, values.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 6.66e-01 | test loss: 9.59e-01 | reg: 2.34e+00 : 100%|████| 2/2 [00:00<00:00,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_time 0.33525586128234863\n",
      "Total number of parameters: 111\n",
      "0.666155 0.95902765\n",
      "17540\n",
      "17540\n",
      "2172\n",
      "2172\n",
      "rmse_train 0.666155 r2_train 0.772232 pcc_train 0.902815 mae_train 0.474798\n",
      "rmse_test 0.959028 r2_test 0.47788 pcc_test 0.784251 mae_test 0.658824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from kan import *\n",
    "import time\n",
    "\n",
    "criterion = F.mse_loss\n",
    "model_kan = KAN(width=[5, 1], grid=5, k=3, seed=args.seed, device=device)\n",
    "print(model_kan.device)\n",
    "start_time = time.time()\n",
    "\n",
    "results = model_kan.train(temp_dataset, opt=\"LBFGS\", steps=2, lamb=0.01, loss_fn=criterion, device=device, lamb_entropy=2.)\n",
    "end_time = time.time()\n",
    "run_time = end_time - start_time\n",
    "print('run_time', run_time) # 1ms per sample\n",
    "total_params = sum(p.numel() for p in model_kan.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(results['train_loss'][-1], results['test_loss'][-1])\n",
    "print(len(results['train_pred']))\n",
    "print(len(results['train_labels']))\n",
    "print(len(results['test_pred']))\n",
    "print(len(results['test_labels']))\n",
    "rmse_train, r2_train, pcc_train, mae_train = scores_metrics(np.array(results['train_pred'][-17540:]), np.array(results['train_labels'][-17540:]))\n",
    "print('rmse_train', rmse_train, 'r2_train', r2_train, 'pcc_train', pcc_train, 'mae_train', mae_train)\n",
    "rmse_test, r2_test, pcc_test, mae_test = scores_metrics(np.array(results['test_pred'][-2172:]), np.array(results['test_labels'][-2172:]))\n",
    "print('rmse_test', rmse_test, 'r2_test', r2_test, 'pcc_test', pcc_test, 'mae_test', mae_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing (0,0,0) with abs, r2=0.9992073178291321\n",
      "fixing (0,1,0) with x^2, r2=0.5925481915473938\n",
      "fixing (0,2,0) with abs, r2=0.9997648000717163\n",
      "fixing (0,3,0) with abs, r2=0.8334081172943115\n",
      "fixing (0,4,0) with exp, r2=0.9240438938140869\n",
      "0.04*(-x_2 - 0.9)**2 - 0.04*Abs(7.6*x_1 + 10.0) + 0.02*Abs(7.78*x_3 + 9.76) - 0.04*Abs(9.98*x_4 - 3.67) + 0.46\n"
     ]
    }
   ],
   "source": [
    "# model_kan.plot()\n",
    "lib = ['x','x^2','exp','log','sqrt','abs']\n",
    "model_kan.auto_symbolic(lib=lib)\n",
    "formula = model_kan.symbolic_formula()\n",
    "print(formula[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_kan.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_kan.save_ckpt('kan_lastmlp.ckpt')\n",
    "# model_kan = KAN(width=[122, 10, 10, 1], grid=5, k=3, seed=args.seed, device=device)\n",
    "# model_kan.load_ckpt('kan_lastmlp.ckpt')\n",
    "# model_kan.train(temp_dataset, opt=\"LBFGS\", steps=1, lamb=0.01, loss_fn=criterion, device=device)\n",
    "# model_kan.plot()\n",
    "\n",
    "# obtaining symbolic formula\n",
    "# formula, variables = model_kan.symbolic_formula()\n",
    "# model_kan.suggest_symbolic(2,0,0)\n",
    "# print(model_kan.symbolic_formula())\n",
    "# formula[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Uni_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
